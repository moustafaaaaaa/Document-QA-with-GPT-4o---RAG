# -*- coding: utf-8 -*-
"""Document QA with GPT-4o - RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13nSQuprce8f1WJ-xKeIuioOkgcC6FShf
"""

!pip install transformers sentence-transformers langchain langchain-community langchain-openai faiss-cpu unstructured unstructured[pdf]

import os

from langchain_openai import ChatOpenAI
from langchain.document_loaders import UnstructuredFileLoader
from langchain_community.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import RetrievalQA

os.environ["OPENAI_API_KEY"] = "your_openai_api_key"

llm = ChatOpenAI(model="gpt-4o", temperature=0)

# load the document
loader = UnstructuredFileLoader("/content/attention_is_all_you_need.pdf")
documents = loader.load()

type(documents[0])

# create text chunks

text_splitter = CharacterTextSplitter(separator='/n',
                                      chunk_size=1000,
                                      chunk_overlap=200)

text_chunks = text_splitter.split_documents(documents)

# loading the vector embedding model
embeddings = HuggingFaceEmbeddings()
# vector embedding for text chunks
knowledge_base = FAISS.from_documents(text_chunks, embeddings)

# chain for aq retrieval
qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=knowledge_base.as_retriever())

question = "What is this document about?"
response = qa_chain.invoke({"query": question})
print(response["result"])

response

question = "What is the model architecture discussed in this paper?"
response = qa_chain.invoke({"query": question})
response["result"]

question = "What are the applications of attention in our Model?"
response = qa_chain.invoke({"query": question})
response["result"]

print(response["result"])

